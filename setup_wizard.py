#!/usr/bin/env python3
"""
AI-Enhanced PDF Extraction System - Setup Wizard
Mit automatischer Hardware-Erkennung und Optimierung
"""

import os
import sys
import json
import requests
import platform
import subprocess
import psutil
from pathlib import Path
from datetime import datetime
from supabase import create_client
import boto3
from sentence_transformers import SentenceTransformer

class AISetupWizard:
    def __init__(self):
        self.config = {}
        self.config_file = Path("config.json")
        self.ollama_base_url = "http://localhost:11434"
        self.hardware_info = None
        
    def welcome_message(self):
        print("=" * 70)
        print("    AI-ENHANCED PDF EXTRACTION SYSTEM - SETUP WIZARD")
        print("=" * 70)
        print("ðŸš€ Automatische Hardware-Erkennung und Optimierung")
        print("âš¡ Apple Silicon, RTX A-Series & Gaming GPUs Support")
        print("ðŸ§  Ollama fÃ¼r intelligentes AI-Chunking")
        print("=" * 70)
        print()
        
    def check_existing_config(self):
        """PrÃ¼ft ob bereits eine gÃ¼ltige Konfiguration existiert"""
        if not self.config_file.exists():
            return False
            
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                existing_config = json.load(f)
            
            # PrÃ¼fe ob wichtige Felder vorhanden sind
            required_fields = ['supabase_url', 'supabase_key', 'r2_account_id', 'embedding_model']
            missing_fields = [field for field in required_fields if field not in existing_config or not existing_config[field]]
            
            if missing_fields:
                print(f"âš ï¸  UnvollstÃ¤ndige Konfiguration gefunden (fehlende Felder: {missing_fields})")
                return False
                
            print("ðŸ“‹ VORHANDENE KONFIGURATION GEFUNDEN")
            print("=" * 50)
            print(f"ðŸ—„ï¸  Datenbank: {existing_config.get('supabase_url', 'N/A')}")
            print(f"â˜ï¸  Storage: Account ID {existing_config.get('r2_account_id', 'N/A')[:8]}...")
            print(f"ðŸ§  AI Model: {existing_config.get('embedding_model', 'N/A')}")
            print(f"âš¡ Hardware: {existing_config.get('performance_boost', 'Standard')}")
            print()
            
            # Test Verbindungen
            print("ðŸ§ª TESTE VORHANDENE KONFIGURATION...")
            print("-" * 30)
            
            # Test Supabase
            try:
                client = create_client(existing_config['supabase_url'], existing_config['supabase_key'])
                client.table("chunks").select("id").limit(1).execute()
                print("âœ… Supabase Verbindung: OK")
                supabase_ok = True
            except Exception as e:
                print(f"âŒ Supabase Verbindung: FEHLER ({e})")
                supabase_ok = False
            
            # Test R2 (falls konfiguriert)
            r2_ok = True
            if all(field in existing_config for field in ['r2_account_id', 'r2_access_key_id', 'r2_secret_access_key']):
                try:
                    s3_client = boto3.client(
                        's3',
                        endpoint_url=f"https://{existing_config['r2_account_id']}.r2.cloudflarestorage.com",
                        aws_access_key_id=existing_config['r2_access_key_id'],
                        aws_secret_access_key=existing_config['r2_secret_access_key']
                    )
                    s3_client.list_objects_v2(Bucket=existing_config.get('r2_bucket_name', 'test'), MaxKeys=1)
                    print("âœ… R2 Storage Verbindung: OK")
                except Exception as e:
                    print(f"âŒ R2 Storage Verbindung: FEHLER ({e})")
                    r2_ok = False
            else:
                print("âš ï¸  R2 Storage: Nicht vollstÃ¤ndig konfiguriert")
                r2_ok = False
            
            print()
            
            if supabase_ok and r2_ok:
                print("âœ¨ KONFIGURATION VOLLSTÃ„NDIG FUNKTIONSFÃ„HIG!")
                choice = input("MÃ¶chten Sie die vorhandene Konfiguration verwenden? (j/N): ").strip().lower()
                if choice == 'j':
                    self.config = existing_config
                    print("âœ… Vorhandene Konfiguration wird verwendet")
                    return True
            else:
                print("âš ï¸  KONFIGURATION HAT PROBLEME")
                choice = input("MÃ¶chten Sie die Konfiguration trotzdem verwenden? (j/N): ").strip().lower()
                if choice == 'j':
                    self.config = existing_config
                    print("âœ… Vorhandene Konfiguration wird verwendet (mit bekannten Problemen)")
                    return True
                    
            print("ðŸ”§ Starte neue Konfiguration...")
            return False
            
        except Exception as e:
            print(f"âŒ Fehler beim Lesen der Konfiguration: {e}")
            return False
        
    def select_providers(self):
        """Provider-Auswahl fÃ¼r Database und Storage"""
        print("ðŸ”§ PROVIDER AUSWAHL")
        print("=" * 50)
        print()
        
        # Database Provider Selection
        print("ðŸ“Š DATENBANK PROVIDER")
        print("-" * 30)
        print("1. Supabase (empfohlen fÃ¼r Einsteiger)")
        print("2. AWS RDS PostgreSQL")
        print("3. Google Cloud SQL")
        print("4. Azure Database for PostgreSQL")
        print("5. Self-hosted PostgreSQL")
        print()
        
        while True:
            db_choice = input("Datenbank Provider wÃ¤hlen (1-5): ").strip()
            if db_choice in ['1', '2', '3', '4', '5']:
                break
            print("âŒ Bitte 1-5 wÃ¤hlen")
        
        database_providers = {
            '1': 'supabase',
            '2': 'aws_rds', 
            '3': 'google_cloud_sql',
            '4': 'azure_postgresql',
            '5': 'self_hosted'
        }
        database_provider = database_providers[db_choice]
        
        print()
        print("â˜ï¸ CLOUD STORAGE PROVIDER")
        print("-" * 30)
        print("1. Cloudflare R2 (empfohlen)")
        print("2. AWS S3")
        print("3. Google Cloud Storage")
        print("4. Azure Blob Storage")
        print("5. MinIO / Self-hosted S3")
        print()
        
        while True:
            storage_choice = input("Storage Provider wÃ¤hlen (1-5): ").strip()
            if storage_choice in ['1', '2', '3', '4', '5']:
                break
            print("âŒ Bitte 1-5 wÃ¤hlen")
        
        storage_providers = {
            '1': 'cloudflare_r2',
            '2': 'aws_s3',
            '3': 'google_cloud_storage', 
            '4': 'azure_blob',
            '5': 'minio_s3'
        }
        storage_provider = storage_providers[storage_choice]
        
        print()
        print(f"âœ… GewÃ¤hlt: {database_provider} + {storage_provider}")
        print()
        
        return database_provider, storage_provider
        
    def detect_hardware(self):
        """Detaillierte Hardware-Erkennung"""
        print("ðŸ”§ HARDWARE-ANALYSE")
        print("-" * 30)
        
        info = {
            "platform": platform.system(),
            "processor": platform.processor(),
            "cpu_count": psutil.cpu_count(logical=False),
            "cpu_count_logical": psutil.cpu_count(logical=True),
            "ram_gb": round(psutil.virtual_memory().total / (1024**3), 1),
            "gpu": self.detect_gpu(),
            "is_m1_mac": self.is_apple_silicon(),
            "supports_cuda": self.supports_cuda()
        }
        
        print(f"ðŸ’» System: {info['platform']} ({info['processor']})")
        print(f"ðŸ§  CPU: {info['cpu_count']} Cores ({info['cpu_count_logical']} Threads)")
        print(f"ðŸ’¾ RAM: {info['ram_gb']} GB")
        
        gpu = info['gpu']
        if gpu['type'] != 'none':
            print(f"ðŸŽ® GPU: {gpu['name']}")
            if 'memory_gb' in gpu:
                print(f"ðŸ“Š VRAM: {gpu['memory_gb']} GB")
        
        # VerfÃ¼gbare Beschleunigungen anzeigen
        accelerations = []
        if info['is_m1_mac']:
            accelerations.extend(["Apple Silicon", "Metal", "Neural Engine"])
        if info['supports_cuda']:
            accelerations.extend(["NVIDIA CUDA", "TensorRT"])
        
        if accelerations:
            print(f"âš¡ Beschleunigung: {', '.join(accelerations)}")
        else:
            print("ðŸ’» Standard CPU Verarbeitung")
            
        print()
        self.hardware_info = info
        return info
    
    def detect_gpu(self) -> dict:
        """GPU-Erkennung fÃ¼r verschiedene Plattformen"""
        gpu_info = {"type": "none", "memory_gb": 0, "name": ""}
        
        try:
            # NVIDIA GPU Erkennung
            result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', 
                                   '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                if lines and lines[0]:
                    parts = lines[0].split(', ')
                    gpu_name = parts[0].strip()
                    gpu_memory = round(int(parts[1]) / 1024, 1)
                    
                    gpu_info = {
                        "type": "nvidia",
                        "name": gpu_name,
                        "memory_gb": gpu_memory,
                        "supports_cuda": True,
                        "is_workstation": "A" in gpu_name and any(x in gpu_name for x in ["A2000", "A4000", "A5000", "A6000"])
                    }
                    
                    print(f"ðŸŽ® NVIDIA GPU erkannt: {gpu_name} ({gpu_memory} GB)")
                    if gpu_info["is_workstation"]:
                        print("   ðŸ¢ Workstation-Class GPU erkannt!")
        except:
            pass
        
        # Apple Silicon Erkennung
        if self.is_apple_silicon():
            try:
                result = subprocess.run(['system_profiler', 'SPDisplaysDataType'], 
                                      capture_output=True, text=True, timeout=10)
                if "Apple" in result.stdout:
                    gpu_info = {
                        "type": "apple_silicon",
                        "name": "Apple Silicon GPU",
                        "memory_gb": 16,  # Unified Memory
                        "supports_metal": True,
                        "supports_npu": True
                    }
                    print("ðŸŽ Apple Silicon GPU erkannt (Unified Memory)")
            except:
                pass
        
        return gpu_info
    
    def is_apple_silicon(self) -> bool:
        """PrÃ¼ft ob Apple Silicon (M1/M2/M3)"""
        if platform.system() != "Darwin":
            return False
        try:
            result = subprocess.run(['uname', '-m'], capture_output=True, text=True)
            return result.stdout.strip() == "arm64"
        except:
            return False
    
    def supports_cuda(self) -> bool:
        """PrÃ¼ft CUDA Support"""
        try:
            result = subprocess.run(['nvidia-smi'], capture_output=True, timeout=5)
            return result.returncode == 0
        except:
            return False
    
    def determine_optimal_ai_config(self):
        """Bestimmt optimale AI-Konfiguration basierend auf Hardware"""
        print("ðŸ§  AI-KONFIGURATION OPTIMIEREN")
        print("-" * 30)
        
        if not self.hardware_info:
            self.detect_hardware()
        
        # Apple Silicon Optimierung
        if self.hardware_info["is_m1_mac"]:
            config = {
                "vision_model": "llava:7b",
                "text_model": "llama3.1:8b",
                "embedding_model": "embeddinggemma",
                "use_metal_acceleration": True,
                "parallel_workers": min(8, self.hardware_info["cpu_count_logical"]),
                "batch_size": 150,
                "memory_optimization": "unified_memory",
                "performance_boost": "30-50% durch Metal + Neural Engine"
            }
            print("ðŸŽ Apple Silicon Optimierung:")
            print("   âœ… Metal Performance Shaders")
            print("   âœ… Neural Engine fÃ¼r Embeddings")
            print("   âœ… Unified Memory Optimierung")
            
        # NVIDIA GPU Optimierung
        elif self.hardware_info["gpu"]["type"] == "nvidia":
            gpu = self.hardware_info["gpu"]
            
            # RTX A-Series Workstation
            if gpu.get("is_workstation", False):
                if "A6000" in gpu["name"] or "A5000" in gpu["name"]:
                    config = {
                        "vision_model": "llava:7b",  # Optimiert fÃ¼r Speed & Effizienz
                        "text_model": "llama3.1:8b", 
                        "embedding_model": "embeddinggemma",
                        "use_cuda_acceleration": True,
                        "parallel_workers": min(16, self.hardware_info["cpu_count_logical"]),
                        "batch_size": 200,
                        "gpu_memory_fraction": 0.8,
                        "memory_optimization": "workstation_optimized",
                        "performance_boost": "60-90% durch CUDA + Workstation"
                    }
                    print(f"ðŸ¢ {gpu['name']} Workstation Optimierung:")
                    print("   âœ… ECC Memory Support")
                    print("   âœ… Professional Drivers")
                    print("   âœ… 24/7 Dauerbetrieb optimiert")
                    
                elif "A4000" in gpu["name"]:
                    config = {
                        "vision_model": "llava:7b",  # Memory-optimiert fÃ¼r A4000
                        "text_model": "llama3.1:8b",
                        "embedding_model": "embeddinggemma",
                        "use_cuda_acceleration": True,
                        "parallel_workers": min(12, self.hardware_info["cpu_count_logical"]),
                        "batch_size": 180,
                        "gpu_memory_fraction": 0.75,
                        "memory_optimization": "workstation_balanced",
                        "performance_boost": "50-70% durch CUDA + 16GB VRAM"
                    }
                    print("ðŸ¢ RTX A4000 Optimierung:")
                    print("   âœ… 16GB VRAM optimal genutzt")
                    print("   âœ… Workstation StabilitÃ¤t")
                    
                elif "A2000" in gpu["name"]:
                    config = {
                        "vision_model": "llava:7b",
                        "text_model": "llama3.1:8b",
                        "embedding_model": "embeddinggemma",
                        "use_cuda_acceleration": True,
                        "parallel_workers": min(8, self.hardware_info["cpu_count_logical"]),
                        "batch_size": 120,
                        "gpu_memory_fraction": 0.7,
                        "memory_optimization": "vram_conservative",
                        "performance_boost": "40-60% durch CUDA + Memory-Effizienz"
                    }
                    print("ðŸ¢ RTX A2000 Optimierung:")
                    print("   âœ… Memory-effiziente Konfiguration")
                    print("   âœ… Workstation StabilitÃ¤t")
            
            # Gaming RTX GPUs
            elif gpu["memory_gb"] >= 12:
                config = {
                    "vision_model": "llava:7b",  # Schneller und effizienter
                    "text_model": "llama3.1:8b",
                    "embedding_model": "embeddinggemma",
                    "use_cuda_acceleration": True,
                    "parallel_workers": min(12, self.hardware_info["cpu_count_logical"]),
                    "batch_size": 200,
                    "gpu_memory_fraction": 0.8,
                    "memory_optimization": "gpu_optimized",
                    "performance_boost": "50-80% durch CUDA Gaming GPU"
                }
                print(f"ðŸŽ® {gpu['name']} Gaming GPU Optimierung:")
                print("   âœ… High-End Gaming Performance")
                print("   âœ… GroÃŸe Models unterstÃ¼tzt")
            
            else:
                config = {
                    "vision_model": "llava:7b",
                    "text_model": "llama3.1:8b",
                    "embedding_model": "embeddinggemma",
                    "use_cuda_acceleration": True,
                    "parallel_workers": min(10, self.hardware_info["cpu_count_logical"]),
                    "batch_size": 150,
                    "gpu_memory_fraction": 0.7,
                    "memory_optimization": "balanced",
                    "performance_boost": "40-60% durch CUDA"
                }
                print(f"ðŸŽ® {gpu['name']} Standard GPU Optimierung:")
        
        # CPU-Only Optimierung
        else:
            config = {
                "vision_model": "llava:7b",
                "text_model": "llama3.1:8b",
                "embedding_model": "embeddinggemma",
                "parallel_workers": min(self.hardware_info["cpu_count_logical"], 8),
                "batch_size": 100,
                "use_cpu_optimization": True,
                "memory_optimization": "cpu_efficient",
                "performance_boost": "20-30% durch Multi-Threading"
            }
            print("ðŸ’» CPU-Optimierung:")
            print("   âœ… Multi-Threading optimiert")
            print("   âœ… Memory-effizient")
        
        print(f"ðŸ“ˆ Erwartete Performance: {config['performance_boost']}")
        print()
        
        return config
        
    def check_ollama_installation(self):
        print("ðŸ¤– OLLAMA INSTALLATION PRÃœFEN")
        print("-" * 30)
        
        try:
            response = requests.get(f"{self.ollama_base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                installed_models = [model['name'] for model in models]
                
                print("âœ… Ollama lÃ¤uft erfolgreich!")
                print(f"   Installierte Modelle: {len(installed_models)}")
                for model in installed_models:
                    print(f"   - {model}")
                
                return installed_models
            else:
                print("âŒ Ollama lÃ¤uft, aber API nicht erreichbar")
                return []
                
        except requests.exceptions.RequestException:
            print("âŒ Ollama ist nicht verfÃ¼gbar!")
            print("   Bitte starten Sie Ollama mit: 'ollama serve'")
            return []
    
    def setup_required_models(self, installed_models, ai_config):
        print("ðŸ“¥ HARDWARE-OPTIMIERTE MODELLE SETUP")
        print("-" * 30)
        
        # Empfohlene Modelle basierend auf Hardware
        recommended_vision = ai_config.get("vision_model", "llava:7b")
        recommended_text = ai_config.get("text_model", "llama3.1:8b")
        recommended_embedding = "embeddinggemma"  # Always use embeddinggemma
        
        print(f"ðŸŽ¯ Empfohlene Konfiguration fÃ¼r Ihre Hardware:")
        print(f"   Vision Model: {recommended_vision}")
        print(f"   Text Model: {recommended_text}")
        print(f"   Embedding Model: {recommended_embedding}")
        
        if self.hardware_info["is_m1_mac"]:
            print("   ðŸŽ Apple Silicon optimiert")
        elif self.hardware_info["gpu"]["type"] == "nvidia":
            gpu_name = self.hardware_info["gpu"]["name"]
            if "A" in gpu_name:
                print(f"   ðŸ¢ {gpu_name} Workstation optimiert")
            else:
                print(f"   ðŸŽ® {gpu_name} Gaming optimiert")
        else:
            print("   ðŸ’» CPU optimiert")
        
        print()
        
        required_models = {
            recommended_text: "Text Analysis und Semantic Boundary Detection",
            recommended_vision: "Hardware-optimiertes Vision Model",
            recommended_embedding: "Embedding Model fÃ¼r Vektor-Suche"
        }
        
        missing_models = []
        
        for model, description in required_models.items():
            if not any(model in installed for installed in installed_models):
                print(f"âš ï¸  Fehlt: {model} - {description}")
                missing_models.append(model)
            else:
                print(f"âœ… VerfÃ¼gbar: {model}")
        
        if missing_models:
            print(f"\nðŸ“¥ {len(missing_models)} Hardware-optimierte Modelle werden heruntergeladen")
            
            # Automatische Installation basierend auf Hardware-Empfehlung
            auto_install = input("Empfohlene Modelle automatisch installieren? (j/n): ")
            if auto_install.lower() == 'j':
                # Setze AI Config Models
                self.config['vision_model'] = recommended_vision
                self.config['text_model'] = recommended_text
                return self.download_models(missing_models)
            else:
                # Manuelle Auswahl
                return self.manual_model_selection(missing_models)
        
        # Setze verfÃ¼gbare Modelle
        self.config['vision_model'] = recommended_vision
        self.config['text_model'] = recommended_text
        self.config['embedding_model'] = recommended_embedding
        
        print("âœ… Alle empfohlenen Modelle verfÃ¼gbar!")
        return True
    
    def manual_model_selection(self, missing_models):
        """Manuelle Model-Auswahl falls automatisch abgelehnt"""
        print("\nðŸ”§ MANUELLE MODEL-AUSWAHL")
        print("-" * 30)
        
        # Vision Model Auswahl  
        vision_options = ["llava:7b", "llava:7b"]
        print("Vision Model wÃ¤hlen:")
        print("1. llava:7b (Schnell & Effizient, 4GB)")
        print("2. llava:7b (Kompakt, 4GB)")
        
        if self.hardware_info["gpu"]["type"] == "nvidia" and self.hardware_info["gpu"]["memory_gb"] >= 8:
            print("   ðŸŽ¯ Empfohlen fÃ¼r Ihre GPU: llava:7b")
            default_choice = "1"
        else:
            print("   ðŸŽ¯ Empfohlen fÃ¼r Ihre Hardware: llava:7b")
            default_choice = "2"
        
        choice = input(f"Auswahl (1-2, Enter fÃ¼r Empfehlung): ") or default_choice
        
        if choice == "1":
            vision_model = "llava:7b"  # Default auf schnelleres Model
        else:
            vision_model = "llava:7b"
        
        self.config['vision_model'] = vision_model
        self.config['text_model'] = "llama3.1:8b"
        
        # Download ausgewÃ¤hlte Modelle
        models_to_download = [model for model in [vision_model, "llama3.1:8b"] if model in missing_models]
        
        if models_to_download:
            return self.download_models(models_to_download)
        return True
    
    def download_models(self, models):
        print("ðŸ“¥ MODELLE HERUNTERLADEN...")
        print("-" * 30)
        
        for model in models:
            print(f"â³ Lade {model} herunter...")
            try:
                response = requests.post(
                    f"{self.ollama_base_url}/api/pull",
                    json={"name": model},
                    timeout=1800  # 30 Minuten Timeout
                )
                
                if response.status_code == 200:
                    print(f"âœ… {model} erfolgreich installiert!")
                else:
                    print(f"âŒ Fehler beim Download von {model}")
                    return False
                    
            except requests.exceptions.RequestException as e:
                print(f"âŒ Download Fehler fÃ¼r {model}: {e}")
                return False
        
        return True
    
    def test_ollama_models(self):
        print("ðŸ§ª MODELLE TESTEN...")
        print("-" * 30)
        
        # Ensure models are set
        if 'text_model' not in self.config:
            self.config['text_model'] = "llama3.1:8b"
        if 'vision_model' not in self.config:
            self.config['vision_model'] = "llava:7b"  # Aktualisierter Default
        
        # Test Text Model
        try:
            response = requests.post(
                f"{self.ollama_base_url}/api/generate",
                json={
                    "model": self.config['text_model'],
                    "prompt": "Test: What is a service manual?",
                    "stream": False
                },
                timeout=30
            )
            
            if response.status_code == 200:
                print(f"âœ… Text Model ({self.config['text_model']}) funktioniert!")
            else:
                print(f"âŒ Text Model Test fehlgeschlagen")
                return False
                
        except Exception as e:
            print(f"âŒ Text Model Fehler: {e}")
            return False
        
        # Test Vision Model  
        try:
            response = requests.post(
                f"{self.ollama_base_url}/api/generate", 
                json={
                    "model": self.config['vision_model'],
                    "prompt": "Describe this image briefly.",
                    "images": ["iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8/5+hHgAHggJ/PchI7wAAAABJRU5ErkJggg=="],
                    "stream": False
                },
                timeout=30
            )
            
            if response.status_code == 200:
                print(f"âœ… Vision Model ({self.config['vision_model']}) funktioniert!")
            else:
                print(f"âŒ Vision Model Test fehlgeschlagen")
                return False
                
        except Exception as e:
            print(f"âŒ Vision Model Fehler: {e}")
            return False
        
        return True
    
    def collect_ai_config(self):
        print("âš™ï¸  AI PROCESSING KONFIGURATION")
        print("-" * 30)
        print("Konfigurieren Sie die AI-Enhanced PDF Verarbeitung:")
        print()
        
        print("ðŸ§  CHUNKING STRATEGY:")
        print("   â€¢ intelligent = AI wÃ¤hlt beste Methode automatisch (EMPFOHLEN)")
        print("   â€¢ procedure_aware = Fokus auf Reparaturschritte")
        print("   â€¢ error_grouping = Fokus auf Fehlercodes")
        print("   â€¢ semantic = Basis semantische Aufteilung")
        chunk_strategy = input("Chunking Strategy [intelligent]: ").strip() or "intelligent"
        self.config['chunking_strategy'] = chunk_strategy
        print(f"âœ… GewÃ¤hlt: {chunk_strategy}")
        print()
        
        print("ðŸ‘ï¸  VISION ANALYSIS:")
        print("   â€¢ j = Vision AI analysiert PDF-Seiten visuell (EMPFOHLEN)")
        print("   â€¢ n = Nur Text-basierte Analyse")
        print("   âžœ Vision AI erkennt Tabellen, Diagramme, Verfahrensschritte automatisch")
        vision_analysis = input("Vision Analysis aktivieren? (j/n) [j]: ").strip() or "j"
        self.config['use_vision_analysis'] = vision_analysis.lower() == 'j'
        status = "âœ… AKTIVIERT" if self.config['use_vision_analysis'] else "âŒ DEAKTIVIERT"
        print(f"   {status}")
        print()
        
        print("ðŸŽ¯ SEMANTIC BOUNDARY DETECTION:")
        print("   â€¢ j = LLM findet optimale Teilungspunkte (EMPFOHLEN)")
        print("   â€¢ n = Einfache grÃ¶ÃŸenbasierte Teilung")
        print("   âžœ Verhindert das Trennen von zusammengehÃ¶rigen Inhalten")
        semantic_boundaries = input("LLM Semantic Boundary Detection? (j/n) [j]: ").strip() or "j"
        self.config['use_semantic_boundaries'] = semantic_boundaries.lower() == 'j'
        status = "âœ… AKTIVIERT" if self.config['use_semantic_boundaries'] else "âŒ DEAKTIVIERT"
        print(f"   {status}")
        print()
        
        print("ðŸ“ CHUNK-GRÃ–SSENKONFIGURATION:")
        print("   â€¢ Max Chunk Size = Maximale TextlÃ¤nge pro Segment")
        print("   â€¢ Min Chunk Size = Minimale TextlÃ¤nge pro Segment")
        print("   âžœ 600/200 sind optimiert fÃ¼r Service Manuals")
        max_chunk_size = input("Max Chunk Size [600]: ").strip() or "600"
        self.config['max_chunk_size'] = int(max_chunk_size)
        
        min_chunk_size = input("Min Chunk Size [200]: ").strip() or "200"
        self.config['min_chunk_size'] = int(min_chunk_size)
        
        print("âœ… AI Konfiguration abgeschlossen:")
        print(f"   ðŸ§  Strategy: {self.config['chunking_strategy']}")
        print(f"   ðŸ‘ï¸  Vision: {'An' if self.config['use_vision_analysis'] else 'Aus'}")
        print(f"   ðŸŽ¯ Semantic: {'An' if self.config['use_semantic_boundaries'] else 'Aus'}")
        print(f"   ðŸ“ Chunk-GrÃ¶ÃŸe: {self.config['min_chunk_size']}-{self.config['max_chunk_size']}")
        print()
    
    def collect_supabase_config(self):
        print("ðŸ—„ï¸  SUPABASE VECTOR DATABASE KONFIGURATION")
        print("-" * 30)
        print("Supabase speichert die verarbeiteten PDF-Chunks fÃ¼r AI-Suche:")
        print("   â€¢ Kostenloser Account: https://supabase.com")
        print("   â€¢ Erstellen Sie ein neues Projekt")
        print("   â€¢ Kopieren Sie URL und Service Role Key")
        print("   â€¢ ODER verwenden Sie Demo-Werte zum Testen")
        print()
        
        supabase_url = input("Supabase URL (oder 'demo' fÃ¼r Test): ").strip()
        while supabase_url and not supabase_url.startswith('https://') and supabase_url.lower() != 'demo':
            print("âš ï¸  URL muss mit 'https://' beginnen oder 'demo' fÃ¼r Test")
            supabase_url = input("Supabase URL (oder 'demo' fÃ¼r Test): ").strip()
        
        if supabase_url.lower() == 'demo':
            supabase_url = "https://demo.supabase.co"
            supabase_key = "demo-key"
            print("âœ… Demo-Konfiguration gewÃ¤hlt (nur lokale Verarbeitung)")
        else:
            supabase_key = input("Supabase Service Role Key: ").strip()
            
            print("ðŸ§ª Teste Supabase Verbindung...")
            try:
                client = create_client(supabase_url, supabase_key)
                # Test connection with a real table from our schema
                result = client.table("images").select("id").limit(1).execute()
                print("âœ… Supabase Verbindung erfolgreich!")
            except Exception as e:
                print(f"âš ï¸  Supabase Test fehlgeschlagen: {e}")
                print("   Konfiguration wird trotzdem gespeichert")
        
        self.config['supabase_url'] = supabase_url
        self.config['supabase_key'] = supabase_key
        print()
    
    def collect_r2_config(self):
        print("â˜ï¸  CLOUDFLARE R2 STORAGE KONFIGURATION")
        print("-" * 30)
        print("R2 speichert extrahierte Bilder aus PDFs:")
        print("   â€¢ Kostenloser Account: https://dash.cloudflare.com")
        print("   â€¢ Erstellen Sie R2 Bucket + API Token")
        print("   â€¢ ODER verwenden Sie Demo-Werte (keine Bilder-Speicherung)")
        print()
        
        use_demo = input("Demo-Werte verwenden? (j/n) [j]: ").strip() or "j"
        
        if use_demo.lower() == 'j':
            self.config['r2_account_id'] = "demo"
            self.config['r2_access_key_id'] = "demo"
            self.config['r2_secret_access_key'] = "demo"
            self.config['r2_bucket_name'] = "demo"
            print("âœ… Demo R2 Konfiguration gewÃ¤hlt (Bilder werden lokal verarbeitet)")
        else:
            r2_account_id = input("R2 Account ID: ").strip()
            r2_access_key_id = input("R2 Access Key ID: ").strip()
            r2_secret_access_key = input("R2 Secret Access Key: ").strip()
            r2_bucket_name = input("R2 Bucket Name: ").strip()
            
            print("ðŸ§ª Teste R2 Verbindung...")
            try:
                r2_client = boto3.client(
                    's3',
                    endpoint_url=f'https://{r2_account_id}.r2.cloudflarestorage.com',
                    aws_access_key_id=r2_access_key_id,
                    aws_secret_access_key=r2_secret_access_key
                )
                
                # Test bucket access
                r2_client.head_bucket(Bucket=r2_bucket_name)
                print("âœ… R2 Storage Verbindung erfolgreich!")
            except Exception as e:
                print(f"âš ï¸  R2 Test fehlgeschlagen: {e}")
                print("   Konfiguration wird trotzdem gespeichert")
            
            self.config['r2_account_id'] = r2_account_id
            self.config['r2_access_key_id'] = r2_access_key_id
            self.config['r2_secret_access_key'] = r2_secret_access_key
            self.config['r2_bucket_name'] = r2_bucket_name
        
        print()
    
    def collect_database_config(self, provider):
        """Provider-spezifische Database-Konfiguration"""
        if provider == 'supabase':
            self.collect_supabase_config()
        else:
            self.collect_generic_database_config(provider)
    
    def collect_storage_config(self, provider):
        """Provider-spezifische Storage-Konfiguration"""
        if provider == 'cloudflare_r2':
            self.collect_r2_config()
        else:
            self.collect_generic_storage_config(provider)
    
    def collect_generic_database_config(self, provider):
        """Universelle Database-Konfiguration fÃ¼r nicht-Supabase Provider"""
        provider_names = {
            'aws_rds': 'AWS RDS PostgreSQL',
            'google_cloud_sql': 'Google Cloud SQL',
            'azure_postgresql': 'Azure Database for PostgreSQL',
            'self_hosted': 'Self-hosted PostgreSQL'
        }
        
        print(f"ðŸ—„ï¸  {provider_names[provider]} KONFIGURATION")
        print("-" * 50)
        print("PostgreSQL Datenbank mit pgvector Extension benÃ¶tigt:")
        print("   â€¢ Database URL (mit Credentials)")
        print("   â€¢ pgvector Extension muss aktiviert sein")
        print("   â€¢ Service Role oder Admin-Berechtigung")
        print()
        
        database_url = input("Database URL (postgres://user:pass@host:port/dbname): ").strip()
        while not database_url.startswith('postgres://'):
            print("âš ï¸  URL muss mit 'postgres://' beginnen")
            database_url = input("Database URL: ").strip()
        
        database_key = input("Service Role Key (optional, Enter wenn in URL): ").strip()
        
        # Mapping fÃ¼r Backward-KompatibilitÃ¤t
        self.config['supabase_url'] = database_url
        self.config['supabase_key'] = database_key or "not_required"
        
        print(f"âœ… {provider_names[provider]} konfiguriert")
        print()
    
    def collect_generic_storage_config(self, provider):
        """Universelle S3-kompatible Storage-Konfiguration"""
        provider_names = {
            'aws_s3': 'AWS S3',
            'google_cloud_storage': 'Google Cloud Storage',
            'azure_blob': 'Azure Blob Storage',
            'minio_s3': 'MinIO S3'
        }
        
        print(f"â˜ï¸  {provider_names[provider]} KONFIGURATION")
        print("-" * 50)
        print("S3-kompatible Storage fÃ¼r PDF-Bilder:")
        print("   â€¢ Access Key & Secret Key")
        print("   â€¢ Bucket Name")
        print("   â€¢ Optional: Custom Endpoint")
        print()
        
        storage_endpoint = ""
        if provider in ['azure_blob', 'minio_s3']:
            storage_endpoint = input("Storage Endpoint URL (z.B. https://account.blob.core.windows.net): ").strip()
        
        access_key = input("Access Key ID: ").strip()
        secret_key = input("Secret Access Key: ").strip()
        bucket_name = input("Bucket Name: ").strip()
        
        # Mapping fÃ¼r Backward-KompatibilitÃ¤t mit R2-Feldern
        self.config['r2_account_id'] = storage_endpoint or f"{provider}_account"
        self.config['r2_access_key_id'] = access_key
        self.config['r2_secret_access_key'] = secret_key
        self.config['r2_bucket_name'] = bucket_name
        self.config['r2_public_domain_id'] = storage_endpoint or f"{provider}_domain"
        
        print(f"âœ… {provider_names[provider]} konfiguriert")
        print()
    
    def collect_processing_config(self):
        print("ðŸ“ PROCESSING KONFIGURATION")
        print("-" * 30)
        print("Konfigurieren Sie den PDF-Verarbeitungspfad:")
        print(f"   â€¢ Aktueller Documents Ordner: {os.path.abspath('Documents')}")
        print("   â€¢ Das System Ã¼berwacht diesen Ordner automatisch")
        print("   â€¢ Neue PDFs werden sofort mit AI verarbeitet")
        print()
        
        documents_path = input(f"Documents Pfad [{os.path.abspath('Documents')}]: ").strip()
        if not documents_path:
            documents_path = os.path.abspath("Documents")
        
        self.config['documents_path'] = documents_path
        
        if not os.path.exists(documents_path):
            create_dir = input(f"Ordner {documents_path} existiert nicht. Erstellen? (j/n): ")
            if create_dir.lower() == 'j':
                os.makedirs(documents_path, exist_ok=True)
                print(f"âœ… Ordner erstellt: {documents_path}")
            else:
                print("âš ï¸  Ordner muss existieren fÃ¼r die Verarbeitung")
        else:
            # Count existing PDFs
            pdf_files = []
            for root, dirs, files in os.walk(documents_path):
                for file in files:
                    if file.lower().endswith('.pdf'):
                        pdf_files.append(file)
            
            if pdf_files:
                print(f"ðŸ“„ Gefunden: {len(pdf_files)} PDF-Dateien im Ordner")
                print("   Diese werden beim Start automatisch verarbeitet")
            else:
                print("ðŸ“„ Noch keine PDF-Dateien im Ordner")
                print("   Kopieren Sie PDFs hierhin fÃ¼r automatische Verarbeitung")
        
        print("âœ… Processing Konfiguration abgeschlossen")
        print()
    
    def test_embedding_model(self):
        print("ðŸ”¤ EMBEDDING MODEL TESTEN...")
        print("-" * 30)
        
        try:
            model = SentenceTransformer('all-MiniLM-L6-v2')
            test_embedding = model.encode("Test sentence for embedding")
            print(f"âœ… Sentence Transformers Model geladen (Dimension: {len(test_embedding)})")
            return True
        except Exception as e:
            print(f"âŒ Embedding Model Fehler: {e}")
            print("   Bitte installieren Sie: pip install sentence-transformers")
            return False
    
    def check_and_organize_parts_catalogs(self):
        """PrÃ¼ft und organisiert Parts Kataloge vor dem Setup"""
        print("\nðŸ”§ DOCUMENT ORGANIZATION & CATALOG CHECK")
        print("=" * 50)
        
        # Erstelle organisierte Struktur
        self._create_document_structure()
        
        # Organisiere Service Manuals
        self._organize_service_manuals()
        
        # Organisiere Parts Catalogs
        self._organize_parts_catalogs()
        
        print("\nâœ… Document Organization abgeschlossen!")
    
    def _create_document_structure(self):
        """Erstellt die komplette Dokument-Struktur"""
        base_docs = Path("Documents")
        
        # Erstelle Hauptordner
        folders = {
            "Service_Manuals": "Service manuals organized by manufacturer and model",
            "Parts_Catalogs": "Parts catalogs with PDF+CSV pairs",
            "Technical_Bulletins": "Technical bulletins and updates",
            "Installation_Guides": "Installation and setup guides",
            "Troubleshooting": "Troubleshooting guides and procedures",
            "Software_Updates": "Firmware and software updates"
        }
        
        for folder, description in folders.items():
            folder_path = base_docs / folder
            folder_path.mkdir(parents=True, exist_ok=True)
            
            # Erstelle README wenn nicht vorhanden
            readme_path = folder_path / "README.md"
            if not readme_path.exists():
                readme_content = f"""# {folder.replace('_', ' ')}

{description}

## Structure:
```
{folder}/
â””â”€â”€ Manufacturer/
    â””â”€â”€ Model_Series/
        â”œâ”€â”€ Model_Document.pdf
        â””â”€â”€ metadata.json
```

## Supported Manufacturers:
- HP / Hewlett Packard
- Canon
- Konica Minolta
- Xerox
- Ricoh
- Brother
- Epson

Documents are automatically organized by manufacturer and model when processed.
"""
                with open(readme_path, "w") as f:
                    f.write(readme_content)
        
        print("ðŸ“ Dokument-Struktur erstellt")
    
    def _organize_service_manuals(self):
        """Organisiert Service Manuals automatisch"""
        old_manual_dir = Path("Documents/Service Manual")
        new_manual_dir = Path("Documents/Service_Manuals")
        
        if not old_manual_dir.exists():
            print("ðŸ“„ Keine Service Manuals im alten Format gefunden")
            return
        
        print("ðŸ“„ Organisiere Service Manuals...")
        
        # Finde alle PDF-Dateien
        pdf_files = list(old_manual_dir.glob("*.pdf"))
        organized_count = 0
        
        for pdf_file in pdf_files:
            try:
                # Extrahiere Hersteller und Modell aus Dateiname
                manufacturer, model_series = self._extract_manufacturer_model_from_filename(pdf_file.name)
                
                if manufacturer and model_series:
                    # Erstelle Zielordner
                    target_dir = new_manual_dir / manufacturer / model_series
                    target_dir.mkdir(parents=True, exist_ok=True)
                    
                    # Verschiebe Datei
                    target_file = target_dir / pdf_file.name
                    if not target_file.exists():
                        pdf_file.rename(target_file)
                        print(f"   âœ… {pdf_file.name} â†’ {manufacturer}/{model_series}/")
                        
                        # Erstelle Metadata
                        self._create_service_manual_metadata(target_file, manufacturer, model_series)
                        organized_count += 1
                    else:
                        print(f"   âš ï¸ {pdf_file.name} bereits vorhanden")
                else:
                    print(f"   âŒ Konnte Hersteller/Modell nicht erkennen: {pdf_file.name}")
                    
            except Exception as e:
                print(f"   âŒ Fehler bei {pdf_file.name}: {e}")
                # Fallback: Kopiere in Generic Ordner
                try:
                    generic_dir = new_manual_dir / "Generic"
                    generic_dir.mkdir(parents=True, exist_ok=True)
                    target_file = generic_dir / pdf_file.name
                    if not target_file.exists():
                        import shutil
                        shutil.copy2(pdf_file, target_file)
                        print(f"   ðŸ“ {pdf_file.name} â†’ Generic/ (Fallback)")
                        organized_count += 1
                except:
                    pass
        
        # LÃ¶sche alten Ordner wenn leer
        if old_manual_dir.exists() and not any(old_manual_dir.iterdir()):
            old_manual_dir.rmdir()
            print(f"   ðŸ—‘ï¸ Alter Service Manual Ordner entfernt")
        
        print(f"   ðŸ“Š {organized_count} Service Manuals organisiert")
    
    def _organize_parts_catalogs(self):
        """Organisiert Parts Kataloge mit dem bestehenden Manager"""
        parts_catalog_dir = Path("Documents/Parts_Catalogs")
        
        # PrÃ¼fe ob Parts Catalog Verzeichnis existiert
        if not parts_catalog_dir.exists():
            parts_catalog_dir.mkdir(parents=True, exist_ok=True)
            print("ðŸ“ Parts Catalog Verzeichnis erstellt")
            print("ðŸ’¡ Legen Sie Ihre Parts Kataloge in 'Documents/Parts_Catalogs/' ab")
            return
        
        # PrÃ¼fe vorhandene Dateien
        pdf_files = list(parts_catalog_dir.rglob("*.pdf"))
        csv_files = list(parts_catalog_dir.rglob("*.csv"))
        
        print(f"ðŸ“Š Parts Catalog Status:")
        print(f"   ðŸ“„ PDF-Dateien: {len(pdf_files)}")
        print(f"   ðŸ“‹ CSV-Dateien: {len(csv_files)}")
        
        if len(pdf_files) == 0 and len(csv_files) == 0:
            print("   ðŸ’¡ Keine Parts Kataloge gefunden")
            return
        
        # FÃ¼hre Parts Catalog Manager aus
        print("   ðŸ¤– Starte Parts Catalog Organization...")
        try:
            # Import Parts Catalog Manager
            import importlib.util
            spec = importlib.util.spec_from_file_location("parts_catalog_manager", 
                                                        Path("parts_catalog_manager.py"))
            if spec and spec.loader:
                pcm_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(pcm_module)
                
                # Logging fÃ¼r Parts Catalog Manager stumm schalten
                import logging
                logging.getLogger('parts_catalog_manager').setLevel(logging.WARNING)
                
                # FÃ¼hre Parts Catalog Manager aus
                manager = pcm_module.PartsCatalogManager()
                
                # 1. Organisiere Dateien (falls unorganisiert)
                manager.organize_existing_files()
                
                # 2. Finde Paare
                pairs = manager.scan_for_pairs()
                print(f"   âœ… {len(pairs)} gÃ¼ltige PDF+CSV Paare gefunden")
                
                # 3. Zeige Ergebnisse
                if pairs:
                    for pair in pairs:
                        print(f"      ðŸ“¦ {pair.manufacturer}/{pair.model}")
                
                # 4. Check Processing Candidates
                candidates = manager.get_processing_candidates()
                if candidates:
                    print(f"   ðŸš€ {len(candidates)} Modelle bereit fÃ¼r AI-Processing")
                    for candidate in candidates:
                        print(f"      ðŸ“‹ {candidate.manufacturer}/{candidate.model}")
                else:
                    print("   âœ… Alle Parts Kataloge bereits verarbeitet")
                    
        except Exception as e:
            print(f"   âŒ Parts Catalog Manager Fehler: {e}")
    
    def _extract_manufacturer_model_from_filename(self, filename: str) -> tuple[str, str]:
        """Extrahiert Hersteller und Modell aus Service Manual Dateinamen"""
        filename_lower = filename.lower()
        
        # HP Patterns
        if filename_lower.startswith('hp_'):
            # HP_E50045_E50145_E52545_SM.pdf -> HP, E50045_E50145_E52545
            parts = filename.replace('.pdf', '').replace('_SM', '').split('_')
            if len(parts) >= 2:
                manufacturer = parts[0].upper()
                model_series = '_'.join(parts[1:])
                return manufacturer, model_series
        
        # Canon Patterns
        elif 'canon' in filename_lower or filename_lower.startswith('ir'):
            if 'imagerunner' in filename_lower or filename_lower.startswith('ir'):
                # Canon_imageRUNNER_C3520i_SM.pdf -> Canon, imageRUNNER_C3520i
                parts = filename.replace('.pdf', '').replace('_SM', '').split('_')
                manufacturer = 'Canon'
                model_series = '_'.join(parts[1:]) if len(parts) > 1 else parts[0]
                return manufacturer, model_series
        
        # Konica Minolta Patterns
        elif 'konica' in filename_lower or 'minolta' in filename_lower or 'bizhub' in filename_lower:
            # KonicaMinolta_bizhub_C451i_SM.pdf -> Konica_Minolta, bizhub_C451i
            parts = filename.replace('.pdf', '').replace('_SM', '').split('_')
            manufacturer = 'Konica_Minolta'
            model_series = '_'.join(parts[1:]) if len(parts) > 1 else parts[0]
            return manufacturer, model_series
        
        # Xerox Patterns
        elif 'xerox' in filename_lower:
            parts = filename.replace('.pdf', '').replace('_SM', '').split('_')
            manufacturer = 'Xerox'
            model_series = '_'.join(parts[1:]) if len(parts) > 1 else parts[0]
            return manufacturer, model_series
        
        # Generic fallback
        parts = filename.replace('.pdf', '').replace('_SM', '').split('_')
        if len(parts) >= 2:
            return parts[0], '_'.join(parts[1:])
        
        return None, None
    
    def _create_service_manual_metadata(self, pdf_path: Path, manufacturer: str, model_series: str):
        """Erstellt Metadata fÃ¼r Service Manual"""
        metadata = {
            "document_type": "Service Manual",
            "manufacturer": manufacturer,
            "model_series": model_series,
            "file_path": str(pdf_path),
            "file_size_bytes": pdf_path.stat().st_size,
            "created_at": datetime.now().isoformat(),
            "processing_status": "pending",
            "document_info": {
                "language": "en",
                "category": "service_manual",
                "priority": "normal"
            }
        }
        
        metadata_path = pdf_path.parent / "metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
    
    def _start_ai_processing(self):
        """Startet das AI-Processing fÃ¼r neue Parts Kataloge"""
        try:
            print("ðŸ¤– Starte AI-PDF-Processor...")
            
            # Starte ai_pdf_processor.py als separaten Prozess
            result = subprocess.run([
                sys.executable, "ai_pdf_processor.py", "--auto-process"
            ], capture_output=True, text=True, timeout=300)  # 5 min timeout
            
            if result.returncode == 0:
                print("âœ… AI-Processing erfolgreich gestartet!")
                print("ðŸ“Š Verarbeitete Dokumente sind jetzt in der Vector Database verfÃ¼gbar")
            else:
                print(f"âŒ AI-Processing Fehler: {result.stderr}")
                print("ðŸ’¡ Versuchen Sie manuell: python ai_pdf_processor.py")
                
        except subprocess.TimeoutExpired:
            print("â±ï¸ AI-Processing lÃ¤uft noch... (wird im Hintergrund fortgesetzt)")
        except FileNotFoundError:
            print("âŒ ai_pdf_processor.py nicht gefunden")
        except Exception as e:
            print(f"âŒ Unerwarteter Fehler beim AI-Processing: {e}")
            print("ðŸ’¡ Versuchen Sie manuell: python ai_pdf_processor.py")
    
    def save_config(self):
        print("ðŸ’¾ KONFIGURATION SPEICHERN")
        print("-" * 30)
        
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… Konfiguration gespeichert: {self.config_file}")
            return True
        except Exception as e:
            print(f"âŒ Fehler beim Speichern: {e}")
            return False
    
    def setup_database_tables(self):
        print("ðŸ—ƒï¸  DATENBANK TABELLEN EINRICHTEN")
        print("-" * 30)
        
        try:
            client = create_client(self.config['supabase_url'], self.config['supabase_key'])
            
            # Check if tables already exist by trying to query them
            try:
                # Test if chunks table exists
                client.table("chunks").select("id").limit(1).execute()
                print("âœ… chunks Tabelle bereits vorhanden")
                
                # Test if images table exists  
                client.table("images").select("id").limit(1).execute()
                print("âœ… images Tabelle bereits vorhanden")
                
                # Test if processing_log table exists
                client.table("processing_log").select("id").limit(1).execute()
                print("âœ… processing_log Tabelle bereits vorhanden")
                
                print("âœ… Alle Datenbank Tabellen sind verfÃ¼gbar!")
                return True
                
            except Exception as table_check_error:
                print("âš ï¸  Einige Tabellen fehlen noch")
                print()
                print("ðŸ“‹ MANUELLE DATENBANK-EINRICHTUNG ERFORDERLICH")
                print("-" * 50)
                print("Die Datenbank-Tabellen mÃ¼ssen manuell Ã¼ber das Supabase Dashboard erstellt werden:")
                print()
                print("1. ðŸŒ Ã–ffnen Sie: https://supabase.com/dashboard")
                print("2. ðŸ“‚ WÃ¤hlen Sie Ihr Projekt aus")
                print("3. ðŸ› ï¸  Gehen Sie zu 'SQL Editor'")
                print("4. ðŸ“„ FÃ¼hren Sie das Schema aus: database_schema.sql")
                print()
                print("ðŸ’¡ TIPP: Das komplette Schema finden Sie in der Datei:")
                print(f"   ðŸ“ {os.path.abspath('database_schema.sql')}")
                print()
                
                # Show if the schema file exists
                if os.path.exists('database_schema.sql'):
                    print("âœ… Schema-Datei gefunden! Sie kÃ¶nnen sie direkt verwenden.")
                else:
                    print("âš ï¸  Schema-Datei nicht gefunden. Erstelle sie...")
                    self.create_schema_file()
                    print("âœ… Schema-Datei erstellt: database_schema.sql")
                
                print()
                choice = input("Haben Sie das Schema bereits installiert? (j/n): ").strip().lower()
                if choice == 'j':
                    print("âœ… Datenbank als konfiguriert markiert")
                    return True
                else:
                    print("â„¹ï¸  Fahren Sie mit dem Setup fort. Das Schema kann spÃ¤ter installiert werden.")
                    return True  # Continue anyway, user can set up schema later
                
        except Exception as e:
            print(f"âŒ Datenbank Setup Fehler: {e}")
            print("   Bitte prÃ¼fen Sie Ihre Supabase-Verbindungseinstellungen")
            return False
    
    def create_schema_file(self):
        """Create a database schema file if it doesn't exist"""
        schema_content = """-- ============================================
-- POSTGRESQL DATABASE SCHEMA (EmbeddingGemma 768D) 
-- Compatible with PostgreSQL databases including Supabase, AWS RDS, etc.
-- Features: 768D EmbeddingGemma vectors, Cloud storage integration, Processing logs
-- ============================================

-- Note: Requires the "vector" extension for pgvector support
-- Install if not available:
CREATE EXTENSION IF NOT EXISTS vector;

-- Create chunks table
CREATE TABLE IF NOT EXISTS chunks (
  id bigint PRIMARY KEY GENERATED ALWAYS AS IDENTITY,
  
  -- CONTENT
  content TEXT NOT NULL,
  embedding vector(768),  -- EmbeddingGemma produces 768-dimensional vectors
  
  -- DOCUMENT METADATA
  manufacturer TEXT,
  document_type TEXT,
  file_path TEXT,
  original_filename TEXT,
  file_hash TEXT,
  
  -- CHUNK METADATA
  chunk_type TEXT,
  page_number INTEGER,
  chunk_index INTEGER,
  
  -- EXTRACTED FEATURES
  error_codes TEXT[],
  figure_references TEXT[],
  connection_points TEXT[],
  procedures TEXT[],
  
  -- SYSTEM
  metadata JSONB,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Create images table
CREATE TABLE IF NOT EXISTS images (
  id bigint PRIMARY KEY GENERATED ALWAYS AS IDENTITY,

  -- IMAGE IDENTIFICATION & HASH
  image_hash text NOT NULL UNIQUE,
  file_hash text NOT NULL,
  original_filename text NOT NULL,
  page_number integer NOT NULL,

  -- CLOUD STORAGE INFO (R2/S3 compatible)
  storage_url text NOT NULL,
  storage_bucket text NOT NULL,
  storage_key text NOT NULL,

  -- IMAGE METADATA
  width integer,
  height integer,
  format text,
  file_size_bytes bigint,

  -- SYSTEM
  metadata JSONB,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Create processing_log table  
CREATE TABLE IF NOT EXISTS processing_log (
  id bigint PRIMARY KEY GENERATED ALWAYS AS IDENTITY,
  
  file_path TEXT NOT NULL,
  file_hash TEXT UNIQUE NOT NULL,
  status TEXT NOT NULL,
  
  chunks_created INTEGER DEFAULT 0,
  images_extracted INTEGER DEFAULT 0,
  error_message TEXT,
  processing_time_seconds REAL,
  
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Create indexes for better performance
CREATE INDEX IF NOT EXISTS idx_chunks_embedding ON chunks USING ivfflat (embedding vector_cosine_ops);
CREATE INDEX IF NOT EXISTS idx_chunks_file_hash ON chunks(file_hash);
CREATE INDEX IF NOT EXISTS idx_chunks_manufacturer ON chunks(manufacturer);
CREATE INDEX IF NOT EXISTS idx_images_file_hash ON images(file_hash);
CREATE INDEX IF NOT EXISTS idx_processing_log_status ON processing_log(status);
"""
        
        try:
            with open('database_schema.sql', 'w', encoding='utf-8') as f:
                f.write(schema_content)
        except Exception as e:
            print(f"âŒ Fehler beim Erstellen der Schema-Datei: {e}")
    
    def final_summary(self):
        print("=" * 70)
        print("    SETUP ERFOLGREICH ABGESCHLOSSEN!")
        print("=" * 70)
        print("âœ… Ollama Models konfiguriert und getestet")
        print("âœ… AI Processing Parameter gesetzt")
        print("âœ… Supabase Vector Database verbunden")
        print("âœ… Cloudflare R2 Storage konfiguriert")
        print("âœ… Embedding Model verfÃ¼gbar")
        print("âœ… Parts Catalog System organisiert")
        print("âœ… Konfiguration in config.json gespeichert")
        
        # Document Organization Info
        docs_base = Path("Documents")
        if docs_base.exists():
            print(f"\nðŸ—‚ï¸ DOCUMENT ORGANIZATION STATUS:")
            
            # Service Manuals
            service_dir = docs_base / "Service_Manuals"
            if service_dir.exists():
                service_pdfs = len(list(service_dir.rglob("*.pdf")))
                service_manufacturers = len([d for d in service_dir.iterdir() if d.is_dir()])
                if service_pdfs > 0:
                    print(f"   ï¿½ Service Manuals: {service_pdfs} PDFs")
                    print(f"   ðŸ­ Hersteller (Service): {service_manufacturers}")
            
            # Parts Catalogs
            parts_dir = docs_base / "Parts_Catalogs" 
            if parts_dir.exists():
                parts_pdfs = len(list(parts_dir.rglob("*.pdf")))
                parts_csvs = len(list(parts_dir.rglob("*.csv")))
                parts_manufacturers = len([d for d in parts_dir.iterdir() 
                                         if d.is_dir() and d.name != '__pycache__'])
                if parts_pdfs > 0 or parts_csvs > 0:
                    print(f"   ðŸ“‹ Parts Kataloge: {parts_pdfs} PDFs, {parts_csvs} CSVs")
                    print(f"   ðŸ­ Hersteller (Parts): {parts_manufacturers}")
                    
                    # Count organized pairs
                    if parts_manufacturers > 0:
                        total_models = 0
                        for mfg_dir in parts_dir.iterdir():
                            if mfg_dir.is_dir() and mfg_dir.name != '__pycache__':
                                models = [d.name for d in mfg_dir.iterdir() if d.is_dir()]
                                total_models += len(models)
                        print(f"   ðŸ“¦ Organisierte Modelle: {total_models}")
        
        print("\nï¿½ðŸš€ NÃ„CHSTE SCHRITTE:")
        print("1. Starten Sie das System: python ai_pdf_processor.py")
        print("2. Neue Dokumente in die organisierten Ordner kopieren:")
        print("   â€¢ Service Manuals â†’ Documents/Service_Manuals/")
        print("   â€¢ Parts Kataloge â†’ Documents/Parts_Catalogs/") 
        print("   â€¢ Andere PDFs â†’ Documents/ (werden automatisch erkannt)")
        print("3. Parts Kataloge werden automatisch verarbeitet")
        print("4. System nutzt automatisch Ihre Hardware-Beschleunigung")
        print("5. n8n Chat Bot fÃ¼r Techniker-Anfragen verfÃ¼gbar")
        
        # PrÃ¼fe ob es Processing Candidates gibt
        parts_dir = Path("Documents/Parts_Catalogs")
        if parts_dir.exists():
            try:
                import importlib.util
                spec = importlib.util.spec_from_file_location("parts_catalog_manager", 
                                                            Path("parts_catalog_manager.py"))
                if spec and spec.loader:
                    pcm_module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(pcm_module)
                    manager = pcm_module.PartsCatalogManager()
                    candidates = manager.get_processing_candidates()
                    
                    if candidates:
                        print(f"\nðŸ¤– OPTIONAL: AI-PROCESSING STARTEN")
                        print(f"Sie haben {len(candidates)} neue Modelle bereit fÃ¼r AI-Processing:")
                        for candidate in candidates[:3]:  # Show max 3
                            print(f"   ðŸ“‹ {candidate.manufacturer}/{candidate.model}")
                        if len(candidates) > 3:
                            print(f"   ... und {len(candidates) - 3} weitere")
                        
                        choice = input(f"\nMÃ¶chten Sie das AI-Processing jetzt starten? (j/N): ").strip().lower()
                        if choice == 'j':
                            print("\nðŸš€ Starte AI-Processing...")
                            self._start_ai_processing()
                        else:
                            print("ðŸ’¡ Sie kÃ¶nnen das AI-Processing spÃ¤ter mit 'python ai_pdf_processor.py' starten")
            except:
                pass  # Silently continue if parts catalog manager not available
        
        print()
        print("ðŸ“Š AI-FEATURES AKTIVIERT:")
        print(f"   ðŸ§  Vision Model: {self.config.get('vision_model', 'N/A')}")
        print(f"   ðŸ’­ Text Model: {self.config.get('text_model', 'N/A')}")
        print(f"   ðŸ‘ï¸  Vision Analysis: {'âœ…' if self.config.get('use_vision_analysis') else 'âŒ'}")
        print(f"   ðŸŽ¯ Semantic Boundaries: {'âœ…' if self.config.get('use_semantic_boundaries') else 'âŒ'}")
        print("=" * 70)
    
    def run_setup(self):
        self.welcome_message()
        
        # 0. PrÃ¼fe vorhandene Konfiguration
        config_exists = self.check_existing_config()
        
        # 0.5. Parts Catalog Check (immer ausfÃ¼hren)
        self.check_and_organize_parts_catalogs()
        
        if config_exists:
            print("\nðŸŽ¯ Setup mit vorhandener Konfiguration abgeschlossen!")
            self.final_summary()
            return
        
        # 1. Provider-Auswahl (falls neue Konfiguration)
        database_provider, storage_provider = self.select_providers()
        
        # 2. Hardware-Analyse
        self.detect_hardware()
        
        # 3. AI-Konfiguration basierend auf Hardware bestimmen
        ai_config = self.determine_optimal_ai_config()
        
        # 4. Ollama Setup
        installed_models = self.check_ollama_installation()
        if not installed_models:
            print("âŒ Setup abgebrochen - Ollama nicht verfÃ¼gbar")
            return False
            
        # 5. Hardware-optimierte Models Setup
        if not self.setup_required_models(installed_models, ai_config):
            print("âŒ Setup abgebrochen - Model Setup fehlgeschlagen")
            return False
            
        # 5. Model Tests
        if not self.test_ollama_models():
            print("âŒ Setup abgebrochen - Model Tests fehlgeschlagen")
            return False
        
        # 6. AI Configuration aus Hardware-Analyse Ã¼bernehmen
        self.config.update(ai_config)
        
        # 7. Parts Catalog Check & Organization
        self.check_and_organize_parts_catalogs()
        
        # 8. Provider-spezifische Cloud Services Configuration
        self.collect_database_config(database_provider)
        self.collect_storage_config(storage_provider)
        self.collect_processing_config()
        
        # 9. Test dependencies
        if not self.test_embedding_model():
            return False
        
        # 10. Save Hardware-optimierte configuration
        if not self.save_config():
            return False
        
        # 11. Setup database
        self.setup_database_tables()
        
        # 12. Hardware-optimierte Summary
        self.final_hardware_summary()
        
        return True
    
    def final_hardware_summary(self):
        """Hardware-spezifische Setup-Zusammenfassung"""
        print("=" * 70)
        print("    HARDWARE-OPTIMIERTES SETUP ABGESCHLOSSEN!")
        print("=" * 70)
        
        # Hardware Info
        if self.hardware_info["is_m1_mac"]:
            print("ðŸŽ APPLE SILICON KONFIGURATION:")
            print("   âœ… Metal Performance Shaders aktiviert")
            print("   âœ… Neural Engine fÃ¼r Embeddings")
            print("   âœ… Unified Memory optimiert")
        elif self.hardware_info["gpu"]["type"] == "nvidia":
            gpu = self.hardware_info["gpu"]
            print(f"ðŸŽ® NVIDIA GPU KONFIGURATION:")
            print(f"   âœ… {gpu['name']} erkannt")
            print(f"   âœ… CUDA Acceleration aktiviert")
            if gpu.get("is_workstation", False):
                print("   âœ… Workstation-Optimierung (ECC + Professional)")
        else:
            print("ðŸ’» CPU-OPTIMIERTE KONFIGURATION:")
            print(f"   âœ… {self.hardware_info['cpu_count_logical']} Threads genutzt")
        
        # AI Models
        print(f"\nðŸ§  AI-MODELLE KONFIGURIERT:")
        print(f"   Vision: {self.config.get('vision_model', 'N/A')}")
        print(f"   Text: {self.config.get('text_model', 'N/A')}")
        print(f"   Parallel Workers: {self.config.get('parallel_workers', 'N/A')}")
        print(f"   Batch Size: {self.config.get('batch_size', 'N/A')}")
        
        # Performance
        performance = self.config.get('performance_boost', 'Optimiert')
        print(f"\nðŸ“ˆ ERWARTETE PERFORMANCE: {performance}")
        
        print("\nâœ… WEITERE KOMPONENTEN:")
        print("   âœ… Supabase Vector Database verbunden")
        print("   âœ… Cloudflare R2 Storage konfiguriert") 
        print("   âœ… Hardware-optimiertes Embedding Model")
        print("   âœ… Parts Catalog System organisiert")
        print("   âœ… Konfiguration in config.json gespeichert")
        
        # Parts Catalog & Document Info
        docs_base = Path("Documents")
        if docs_base.exists():
            print(f"\nðŸ—‚ï¸ DOCUMENT ORGANIZATION STATUS:")
            
            # Service Manuals
            service_dir = docs_base / "Service_Manuals"
            if service_dir.exists():
                service_pdfs = len(list(service_dir.rglob("*.pdf")))
                service_manufacturers = len([d for d in service_dir.iterdir() if d.is_dir()])
                print(f"   ðŸ“„ Service Manuals: {service_pdfs} PDFs")
                print(f"   ðŸ­ Hersteller (Service): {service_manufacturers}")
            
            # Parts Catalogs
            parts_dir = docs_base / "Parts_Catalogs" 
            if parts_dir.exists():
                parts_pdfs = len(list(parts_dir.rglob("*.pdf")))
                parts_csvs = len(list(parts_dir.rglob("*.csv")))
                parts_manufacturers = len([d for d in parts_dir.iterdir() 
                                         if d.is_dir() and d.name != '__pycache__'])
                if parts_pdfs > 0 or parts_csvs > 0:
                    print(f"   ðŸ“‹ Parts Kataloge: {parts_pdfs} PDFs, {parts_csvs} CSVs")
                    print(f"   ðŸ­ Hersteller (Parts): {parts_manufacturers}")
                    
                    # Count organized pairs
                    if parts_manufacturers > 0:
                        total_models = 0
                        for mfg_dir in parts_dir.iterdir():
                            if mfg_dir.is_dir() and mfg_dir.name != '__pycache__':
                                models = [d.name for d in mfg_dir.iterdir() if d.is_dir()]
                                total_models += len(models)
                        print(f"   ðŸ“¦ Organisierte Modelle: {total_models}")
            
            # Other document types
            other_folders = ["Technical_Bulletins", "Installation_Guides", "Troubleshooting"]
            for folder in other_folders:
                folder_path = docs_base / folder
                if folder_path.exists():
                    doc_count = len(list(folder_path.rglob("*.pdf")))
                    if doc_count > 0:
                        print(f"   ï¿½ {folder.replace('_', ' ')}: {doc_count} Dokumente")
        
        print("\nðŸŽ¯ STATUS & NÃ„CHSTE SCHRITTE:")
        
        # PrÃ¼fe ob es Processing Candidates gibt
        parts_dir = Path("Documents/Parts_Catalogs")
        has_candidates = False
        if parts_dir.exists():
            try:
                import importlib.util
                spec = importlib.util.spec_from_file_location("parts_catalog_manager", 
                                                            Path("parts_catalog_manager.py"))
                if spec and spec.loader:
                    pcm_module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(pcm_module)
                    manager = pcm_module.PartsCatalogManager()
                    candidates = manager.get_processing_candidates()
                    
                    if candidates:
                        has_candidates = True
                        print(f"ðŸš€ AI-PROCESSING BEREIT!")
                        print(f"   {len(candidates)} neue Modelle warten auf Verarbeitung:")
                        for candidate in candidates[:3]:  # Show max 3
                            print(f"   ðŸ“‹ {candidate.manufacturer}/{candidate.model}")
                        if len(candidates) > 3:
                            print(f"   ... und {len(candidates) - 3} weitere")
                        
                        choice = input(f"\nMÃ¶chten Sie das AI-Processing jetzt starten? (j/N): ").strip().lower()
                        if choice == 'j':
                            print("\nðŸš€ Starte AI-Processing...")
                            self._start_ai_processing()
                        else:
                            print("ðŸ’¡ Starten Sie spÃ¤ter mit: python ai_pdf_processor.py")
            except:
                pass  # Silently continue if parts catalog manager not available
        
        if not has_candidates:
            print("âœ… SYSTEM EINSATZBEREIT!")
            print("   Alle Dokumente sind verarbeitet und organisiert")
            print("   Nutzen Sie:")
            print("   â€¢ python status.py - fÃ¼r Systemstatus")
            print("   â€¢ python smart_search_engine.py - fÃ¼r Suche")
            print("   â€¢ n8n Chat Bot - fÃ¼r Techniker-Anfragen")
        
        print("=" * 70)

def main():
    wizard = AISetupWizard()
    wizard.run_setup()

if __name__ == "__main__":
    main()
